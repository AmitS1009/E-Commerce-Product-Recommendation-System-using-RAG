# ğŸ›ï¸ E-Commerce Product Recommendation System using RAG

> **Intelligent Document-Based Q&A System** powered by Retrieval Augmented Generation (RAG) for semantic search and natural language queries over product catalogs.

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109-green.svg)](https://fastapi.tiangolo.com/)
[![ChromaDB](https://img.shields.io/badge/ChromaDB-0.4.22-orange.svg)](https://www.trychroma.com/)
[![Ollama](https://img.shields.io/badge/Ollama-llama3.2-purple.svg)](https://ollama.ai/)

---

## ğŸ“‹ Table of Contents

- [âœ¨ Features](#-features)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸ”„ System Flow](#-system-flow)
- [ğŸ› ï¸ Technology Stack](#ï¸-technology-stack)
- [ğŸ“¦ Installation](#-installation)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“š API Documentation](#-api-documentation)
- [ğŸ§ª Testing Guide](#-testing-guide)
- [ğŸ¯ Design Decisions](#-design-decisions)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ”§ Configuration](#-configuration)
- [ğŸ› Troubleshooting](#-troubleshooting)
- [ğŸš€ Future Enhancements](#-future-enhancements)

---

## âœ¨ Features

<table>
<tr>
<td width="50%">

### ğŸ“„ Document Processing
- âœ… Multi-format support (`.txt`, `.md`, `.pdf`)
- âœ… Intelligent semantic chunking
- âœ… Context-preserving overlap
- âœ… Automatic metadata extraction

</td>
<td width="50%">

### ğŸ” Semantic Search
- âœ… Vector-based similarity search
- âœ… 384-dim embeddings
- âœ… Persistent ChromaDB storage
- âœ… Cosine similarity ranking

</td>
</tr>
<tr>
<td width="50%">

### ğŸ¤– AI-Powered Q&A
- âœ… Context-aware responses
- âœ… Source attribution
- âœ… Local LLM (Ollama)
- âœ… RAG pipeline integration

</td>
<td width="50%">

### ğŸŒ RESTful API
- âœ… FastAPI backend
- âœ… Auto-generated docs (Swagger)
- âœ… Request validation
- âœ… Comprehensive error handling

</td>
</tr>
</table>

---

## ğŸ—ï¸ Architecture

```mermaid
flowchart TB
    subgraph Client["ğŸ‘¤ Client Layer"]
        User[User/API Consumer]
    end
    
    subgraph API["ğŸŒ API Layer - FastAPI"]
        Routes[Route Handlers]
        Models[Pydantic Models]
        Validation[Request Validation]
    end
    
    subgraph Services["âš™ï¸ Service Layer"]
        DocProc[Document Processor]
        Embed[Embedding Service]
        VecStore[Vector Store]
        RAG[RAG Pipeline]
    end
    
    subgraph External["ğŸ”Œ External Services"]
        Ollama[Ollama LLM<br/>llama3.2]
        ChromaDB[(ChromaDB<br/>Vector Database)]
        SentTrans[sentence-transformers<br/>Embedding Model]
    end
    
    User -->|HTTP Request| Routes
    Routes --> Validation
    Validation --> Models
    Models --> DocProc
    Models --> RAG
    
    DocProc -->|Extract & Chunk| Embed
    Embed -->|Generate Vectors| SentTrans
    Embed -->|Store| VecStore
    VecStore -->|Persist| ChromaDB
    
    RAG -->|Query| VecStore
    VecStore -->|Retrieve Context| RAG
    RAG -->|Generate Answer| Ollama
    RAG -->|Response| Routes
    Routes -->|JSON Response| User
    
    style Client fill:#e1f5ff
    style API fill:#fff4e1
    style Services fill:#f0e1ff
    style External fill:#e1ffe1
```

---

## ğŸ”„ System Flow

### ğŸ“¤ Document Upload Flow

```mermaid
sequenceDiagram
    participant User
    participant API as FastAPI
    participant DocProc as Document Processor
    participant Embed as Embedding Service
    participant ChromaDB as Vector Store
    
    User->>API: POST /api/documents/upload<br/>(file: product.pdf)
    API->>DocProc: Save & Extract Text
    DocProc->>DocProc: Chunk Text<br/>(500 chars, 100 overlap)
    DocProc->>Embed: Generate Embeddings<br/>[chunk1, chunk2, ...]
    Embed->>Embed: sentence-transformers<br/>(384-dim vectors)
    Embed->>ChromaDB: Store Vectors + Metadata
    ChromaDB-->>API: Success
    API-->>User: 201 Created<br/>{document_id, chunk_count}
```

### ğŸ” Query Flow (RAG Pipeline)

```mermaid
sequenceDiagram
    participant User
    participant API as FastAPI
    participant Embed as Embedding Service
    participant ChromaDB as Vector Store
    participant RAG as RAG Pipeline
    participant Ollama as LLM (llama3.2)
    
    User->>API: POST /api/query<br/>{query: "What are the features?"}
    API->>Embed: Embed Query
    Embed-->>API: Query Vector
    API->>ChromaDB: Semantic Search<br/>(top-k=3)
    ChromaDB-->>API: Retrieved Chunks<br/>[chunk1, chunk2, chunk3]
    API->>RAG: Build Context + Prompt
    RAG->>Ollama: Generate Answer<br/>(context + query)
    Ollama-->>RAG: Generated Response
    RAG-->>API: Answer + Sources
    API-->>User: 200 OK<br/>{answer, sources, timestamp}
```

---

## ğŸ› ï¸ Technology Stack

```mermaid
graph LR
    subgraph Backend["Backend Framework"]
        A[FastAPI 0.109]
        B[Uvicorn ASGI]
    end
    
    subgraph VectorDB["Vector Database"]
        C[ChromaDB 0.4.22]
        D[Persistent Storage]
    end
    
    subgraph AI["AI/ML Components"]
        E[Ollama - llama3.2]
        F[sentence-transformers]
        G[all-MiniLM-L6-v2]
    end
    
    subgraph Processing["Document Processing"]
        H[pypdf - PDF Extraction]
        I[Custom Chunking Logic]
    end
    
    subgraph Validation["Validation & Config"]
        J[Pydantic Models]
        K[pydantic-settings]
    end
    
    style Backend fill:#4CAF50,color:#fff
    style VectorDB fill:#2196F3,color:#fff
    style AI fill:#FF9800,color:#fff
    style Processing fill:#9C27B0,color:#fff
    style Validation fill:#F44336,color:#fff
```

### ğŸ“Š Technology Comparison

| Component | Choice | Alternatives Considered | Why Chosen |
|-----------|--------|------------------------|------------|
| **Backend** | FastAPI | Flask, Django | Modern, fast, auto-docs, async support |
| **Vector DB** | ChromaDB | Pinecone, Weaviate, Milvus | Local-first, no API keys, easy setup |
| **LLM** | Ollama | OpenAI, Hugging Face | Privacy, free, local inference |
| **Embeddings** | all-MiniLM-L6-v2 | all-mpnet-base, ada-002 | Fast, small, good quality |
| **PDF Parser** | pypdf | PyPDF2, pdfplumber | Modern, well-maintained |

---

## ğŸ“¦ Installation

### Prerequisites

<table>
<tr>
<td width="33%">

#### ğŸ Python 3.10+
```bash
python --version
# Output: Python 3.10.x
```

</td>
<td width="33%">

#### ğŸ¦™ Ollama
Download from [ollama.com](https://ollama.com)
```bash
ollama --version
```

</td>
<td width="33%">

#### ğŸ’¾ 4GB+ RAM
Recommended for smooth operation

</td>
</tr>
</table>

### ğŸ”§ Setup Steps

```mermaid
graph LR
    A[1. Clone/Download] --> B[2. Create venv]
    B --> C[3. Install Deps]
    C --> D[4. Pull Ollama Model]
    D --> E[5. Configure Env]
    E --> F[6. Start Server]
    
    style A fill:#e3f2fd
    style B fill:#f3e5f5
    style C fill:#e8f5e9
    style D fill:#fff3e0
    style E fill:#fce4ec
    style F fill:#c8e6c9
```

#### Step 1ï¸âƒ£: Navigate to Project Directory
```bash
cd "d:\ML\Projects\AI\E-Commerce Product Recommendation System"
```

#### Step 2ï¸âƒ£: Create Virtual Environment
```bash
python -m venv venv
.\venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac
```

#### Step 3ï¸âƒ£: Install Dependencies
```bash
pip install -r requirements.txt
```

**Key Dependencies Installed:**
- `fastapi==0.109.0` - Web framework
- `chromadb==0.4.22` - Vector database
- `sentence-transformers==2.3.1` - Embeddings
- `ollama==0.1.6` - LLM client
- `pypdf==4.0.1` - PDF processing
- `numpy==1.26.4` - âš ï¸ **Must be < 2.0 for ChromaDB compatibility**

> **âš ï¸ NumPy Compatibility:** If you encounter NumPy 2.0 errors, see [NUMPY_FIX.md](./NUMPY_FIX.md)

#### Step 4ï¸âƒ£: Setup Ollama
```bash
# Pull the llama3.2 model (one-time setup)
ollama pull llama3.2

# Verify installation
ollama list
# Should show: llama3.2  ...  4.4 GB  ...
```

#### Step 5ï¸âƒ£: Configure Environment (Optional)
```bash
# Copy example config
copy .env.example .env

# Edit .env if needed (defaults work fine!)
```

#### Step 6ï¸âƒ£: Verify Installation
```bash
# Run system check
python test_system.py
```

**Expected Output:**
```
âœ“ Config loaded: Ollama URL=http://localhost:11434, Model=llama3.2
âœ“ Embedding service imported
âœ“ Vector store imported
âœ“ RAG pipeline imported
âœ“ Ollama is available with model: llama3.2
System check complete!
```

---

## ğŸš€ Quick Start

### Start the Server

```bash
uvicorn app.main:app --reload
```

**Server Output:**
```
============================================================
E-Commerce Product Recommendation System - Starting
============================================================
Ollama URL: http://localhost:11434
Ollama Model: llama3.2
ChromaDB Path: ./chroma_db
Embedding Model: sentence-transformers/all-MiniLM-L6-v2
============================================================
âœ“ Upload directory ready: ./uploads
âœ“ ChromaDB initialized: 0 documents, 0 chunks
============================================================
API is ready! Visit http://localhost:8000/docs
============================================================
INFO: Uvicorn running on http://127.0.0.1:8000
```

### Access the API

| Endpoint | URL | Description |
|----------|-----|-------------|
| ğŸ  **Home** | http://localhost:8000 | API information |
| ğŸ“– **Swagger UI** | http://localhost:8000/docs | Interactive API docs |
| ğŸ“š **ReDoc** | http://localhost:8000/redoc | Alternative docs |
| â¤ï¸ **Health Check** | http://localhost:8000/health | System status |

---

## ğŸ“š API Documentation

### ğŸ”— Endpoint Overview

```mermaid
graph TB
    subgraph API["API Endpoints - http://localhost:8000"]
        ROOT["/"]
        HEALTH["/health"]
        DOCS["/docs"]
        
        subgraph Documents["ğŸ“„ Document Management"]
            UPLOAD["/api/documents/upload<br/>POST"]
            LIST["/api/documents<br/>GET"]
            DELETE["/api/documents/{id}<br/>DELETE"]
        end
        
        subgraph Query["ğŸ” Query"]
            QUERY["/api/query<br/>POST"]
        end
    end
    
    style ROOT fill:#e3f2fd
    style HEALTH fill:#c8e6c9
    style DOCS fill:#fff9c4
    style UPLOAD fill:#f8bbd0
    style LIST fill:#d1c4e9
    style DELETE fill:#ffccbc
    style QUERY fill:#b2dfdb
```

---

### 1ï¸âƒ£ Upload Document

**Endpoint:** `POST /api/documents/upload`

**Description:** Upload and index a product document

**Request:**
```bash
curl -X POST "http://localhost:8000/api/documents/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@sample_product.txt"
```

**Response:** `201 Created`
```json
{
  "document_id": "a1b2c3d4-5678-90ab-cdef-1234567890ab",
  "filename": "sample_product.txt",
  "chunk_count": 6,
  "upload_time": "2026-02-09T01:00:00.123456",
  "message": "Document uploaded and indexed successfully"
}
```

**Process Flow:**
```
File Upload â†’ Text Extraction â†’ Chunking (500 chars) â†’ 
Embedding (384-dim) â†’ Store in ChromaDB â†’ Return Metadata
```

---

### 2ï¸âƒ£ Query Documents

**Endpoint:** `POST /api/query`

**Description:** Ask natural language questions about indexed documents

**Request:**
```bash
curl -X POST "http://localhost:8000/api/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are the features of the headphones?",
    "top_k": 3
  }'
```

**Response:** `200 OK`
```json
{
  "answer": "The Premium Wireless Noise-Cancelling Headphones feature industry-leading Active Noise Cancellation (ANC) technology, 30-hour battery life, Bluetooth 5.2 with multipoint connection, premium sound quality with 40mm drivers, comfortable over-ear design with memory foam cushions, touch controls, and speak-to-chat technology.",
  "sources": [
    {
      "document_id": "a1b2c3d4-...",
      "filename": "sample_product.txt",
      "chunk_index": 1,
      "relevance_score": 0.892,
      "content": "Key Features:\n- Industry-leading Active Noise Cancellation (ANC)..."
    }
  ],
  "query": "What are the features of the headphones?",
  "timestamp": "2026-02-09T01:00:30.789012"
}
```

**RAG Process:**
```
Query â†’ Embed â†’ Search ChromaDB â†’ Retrieve Top-K Chunks â†’ 
Build Context â†’ LLM Generation â†’ Return Answer + Sources
```

---

### 3ï¸âƒ£ List Documents

**Endpoint:** `GET /api/documents`

**Description:** Get all indexed documents with metadata

**Request:**
```bash
curl "http://localhost:8000/api/documents"
```

**Response:** `200 OK`
```json
{
  "documents": [
    {
      "document_id": "a1b2c3d4-...",
      "filename": "sample_product.txt",
      "upload_time": "2026-02-09T01:00:00.123456",
      "chunk_count": 6
    }
  ],
  "total_count": 1
}
```

---

### 4ï¸âƒ£ Delete Document

**Endpoint:** `DELETE /api/documents/{document_id}`

**Description:** Remove a document and all its chunks from the vector store

**Request:**
```bash
curl -X DELETE "http://localhost:8000/api/documents/a1b2c3d4-..."
```

**Response:** `200 OK`
```json
{
  "document_id": "a1b2c3d4-...",
  "message": "Successfully deleted document and 6 chunks",
  "success": true
}
```

---

### 5ï¸âƒ£ Health Check

**Endpoint:** `GET /health`

**Description:** Check system status and component availability

**Response:** `200 OK`
```json
{
  "status": "healthy",
  "timestamp": "2026-02-09T01:05:00.000000",
  "ollama_available": true,
  "chroma_available": true
}
```

---

## ğŸ§ª Testing Guide

### ğŸ“ Sample Test Scenario

The project includes `sample_product.txt` with information about wireless headphones.

**Step-by-Step Testing:**

```mermaid
graph LR
    A[1. Upload Document] --> B[2. Verify Upload]
    B --> C[3. Ask Questions]
    C --> D[4. Check Sources]
    D --> E[5. List Documents]
    E --> F[6. Optional: Delete]
    
    style A fill:#c8e6c9
    style B fill:#bbdefb
    style C fill:#f8bbd0
    style D fill:#fff9c4
    style E fill:#d1c4e9
    style F fill:#ffccbc
```

#### 1ï¸âƒ£ Upload Sample Document
```bash
curl -X POST "http://localhost:8000/api/documents/upload" \
  -F "file=@sample_product.txt"
```

#### 2ï¸âƒ£ Test Questions

<table>
<tr>
<th>Question</th>
<th>Command</th>
<th>Expected Answer</th>
</tr>
<tr>
<td>Features</td>
<td>

```bash
curl -X POST "http://localhost:8000/api/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "What features do the headphones have?"}'
```

</td>
<td>ANC, 30-hour battery, Bluetooth 5.2, touch controls, etc.</td>
</tr>
<tr>
<td>Price</td>
<td>

```bash
curl -X POST "http://localhost:8000/api/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "How much do they cost?"}'
```

</td>
<td>$399.99</td>
</tr>
<tr>
<td>Colors</td>
<td>

```bash
curl -X POST "http://localhost:8000/api/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "What colors are available?"}'
```

</td>
<td>Black, Silver, Midnight Blue</td>
</tr>
<tr>
<td>Battery</td>
<td>

```bash
curl -X POST "http://localhost:8000/api/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "What is the battery life?"}'
```

</td>
<td>30 hours (ANC on), 40 hours (ANC off)</td>
</tr>
</table>

---

## ğŸ¯ Design Decisions

### ğŸ“ Chunking Strategy

```mermaid
graph TD
    A[Original Document] -->|Split| B[Chunk 1<br/>500 chars]
    A -->|Split| C[Chunk 2<br/>500 chars]
    A -->|Split| D[Chunk 3<br/>500 chars]
    
    B -->|100 char overlap| C
    C -->|100 char overlap| D
    
    style A fill:#e3f2fd
    style B fill:#c8e6c9
    style C fill:#fff9c4
    style D fill:#f8bbd0
```

**Parameters:**
- **Chunk Size**: 500 characters (updated to 1000 would be even better for some use cases)
- **Overlap**: 100 characters (user adjusted from default 50)
- **Strategy**: Sliding window

**Rationale:**
- âœ… Preserves context across boundaries
- âœ… Balances granularity vs. completeness
- âœ… Prevents information loss
- âœ… Optimal for e-commerce product descriptions

---

### ğŸ¨ Component Selection Rationale

```mermaid
mindmap
  root((Design<br/>Choices))
    ChromaDB
      Local First
      No API Keys
      Persistent
      Simple Setup
    Ollama
      Privacy
      Free
      Local
      Fast
    sentence-transformers
      Lightweight
      Fast
      Quality
      Open Source
    FastAPI
      Modern
      Async
      Auto Docs
      Type Safe
```

---

## ğŸ“ Project Structure

```
E-Commerce Product Recommendation System/
â”‚
â”œâ”€â”€ ğŸ“‚ app/                          # Application package
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py              # Package initializer
â”‚   â”œâ”€â”€ ğŸš€ main.py                  # FastAPI app & startup logic
â”‚   â”œâ”€â”€ âš™ï¸ config.py                # Configuration management
â”‚   â”œâ”€â”€ ğŸ“‹ models.py                # Pydantic request/response models
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ routes/                  # API endpoints
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“¤ documents.py         # Upload, list, delete endpoints
â”‚   â”‚   â””â”€â”€ ğŸ” query.py             # Q&A endpoint
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ services/                # Business logic
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ document_processor.py    # Text extraction & chunking
â”‚   â”‚   â”œâ”€â”€ ğŸ§  embeddings.py            # Vector generation
â”‚   â”‚   â”œâ”€â”€ ğŸ’¾ vector_store.py          # ChromaDB operations
â”‚   â”‚   â””â”€â”€ ğŸ¤– rag_pipeline.py          # RAG implementation
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ utils/                   # Utilities
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â””â”€â”€ ğŸ“‚ file_handlers.py     # File I/O operations
â”‚
â”œâ”€â”€ ğŸ“‚ uploads/                      # Temporary file storage
â”œâ”€â”€ ğŸ“‚ chroma_db/                    # Vector database (auto-created)
â”œâ”€â”€ ğŸ“‚ venv/                         # Virtual environment
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt              # Python dependencies
â”œâ”€â”€ ğŸ“„ .env.example                  # Environment config template
â”œâ”€â”€ ğŸ“„ .gitignore                    # Git exclusions
â”‚
â”œâ”€â”€ ğŸ“– README.md                     # This file
â”œâ”€â”€ ğŸš€ QUICKSTART.md                 # Quick start guide
â”œâ”€â”€ âš ï¸ NUMPY_FIX.md                  # Troubleshooting guide
â”‚
â”œâ”€â”€ ğŸ“ sample_product.txt            # Test document
â””â”€â”€ ğŸ§ª test_system.py                # System verification script
```

**Code Statistics:**
- **Total Python Files**: 18
- **Total Lines of Code**: ~1200+
- **Core Services**: 4
- **API Endpoints**: 5
- **Pydantic Models**: 8

---

## ğŸ”§ Configuration

### Environment Variables

```bash
# .env file (optional - defaults work!)

# Ollama Settings
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# ChromaDB Settings
CHROMA_PERSIST_DIR=./chroma_db
CHROMA_COLLECTION_NAME=ecommerce_docs

# Embedding Model
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Document Processing
CHUNK_SIZE=500
CHUNK_OVERLAP=100           # â† User customized from 50

# Retrieval Settings
TOP_K_RESULTS=3

# API Settings
API_HOST=0.0.0.0
API_PORT=8000
```

### Configuration Flow

```mermaid
graph LR
    A[.env file] -->|Load| B[pydantic-settings]
    C[Environment Variables] -->|Override| B
    D[Default Values] -->|Fallback| B
    B --> E[settings object]
    E --> F[Application]
    
    style A fill:#e3f2fd
    style B fill:#c8e6c9
    style C fill:#fff9c4
    style D fill:#f8bbd0
    style E fill:#d1c4e9
    style F fill:#b2dfdb
```

---

## ğŸ› Troubleshooting

### Common Issues & Solutions

<table>
<tr>
<th>âš ï¸ Issue</th>
<th>âœ… Solution</th>
</tr>
<tr>
<td>

**NumPy 2.0 Compatibility Error**
```
AttributeError: np.float_ was removed
```

</td>
<td>

```bash
pip install --force-reinstall numpy==1.26.4
```
See [NUMPY_FIX.md](./NUMPY_FIX.md)

</td>
</tr>
<tr>
<td>

**Ollama Connection Failed**
```
Ollama not available
```

</td>
<td>

```bash
# Start Ollama
ollama serve

# Pull model
ollama pull llama3.2
```

</td>
</tr>
<tr>
<td>

**Port 8000 Already in Use**

</td>
<td>

```bash
# Use different port
uvicorn app.main:app --port 8001
```

</td>
</tr>
<tr>
<td>

**Module Import Errors**

</td>
<td>

```bash
# Activate venv
.\venv\Scripts\activate

# Reinstall deps
pip install -r requirements.txt
```

</td>
</tr>
</table>

---

## ğŸš€ Future Enhancements

### Roadmap

```mermaid
timeline
    title Development Roadmap
    section Phase 1 âœ…
        Core RAG System : Document Upload
                        : Semantic Search
                        : Q&A Generation
                        : API Endpoints
    section Phase 2 ğŸ”„
        Enhanced Processing : .docx support
                           : .html support
                           : Image extraction
                           : Table parsing
    section Phase 3 ğŸ“‹
        Advanced Features : Conversational memory
                         : Multi-turn dialogue
                         : Query refinement
                         : Answer ranking
    section Phase 4 ğŸš€
        Production Ready : Authentication
                        : Rate limiting
                        : Monitoring
                        : Caching layer
```

### Potential Improvements

| Feature | Priority | Complexity | Impact |
|---------|----------|------------|--------|
| ğŸ” User Authentication | High | Medium | Security |
| ğŸ’¬ Conversational Memory | High | High | UX |
| ğŸ“Š Analytics Dashboard | Medium | Medium | Insights |
| ğŸŒ Multi-language Support | Medium | High | Global Reach |
| ğŸ¨ Web UI | Medium | Medium | Accessibility |
| âš¡ Redis Caching | Low | Low | Performance |
| ğŸ³ Docker Deployment | Low | Low | DevOps |
| ğŸ“ˆ A/B Testing | Low | Medium | Optimization |

---

## ğŸ“„ License

This project is created as an assignment submission for educational purposes.

---

## ğŸ‘¤ Author

Developed as part of an AI/ML technical assessment showcasing:
- âœ… RAG implementation skills
- âœ… Vector database integration
- âœ… LLM application development
- âœ… Clean code practices
- âœ… Production-ready architecture

---

## ğŸ™ Acknowledgments

- **FastAPI** - Modern web framework
- **ChromaDB** - Vector database solution
- **Ollama** - Local LLM inference
- **sentence-transformers** - Embedding models
- **Hugging Face** - Model repository

---

<div align="center">

### ğŸŒŸ System Status: Production Ready! ğŸŒŸ

**Built with â¤ï¸ using Python, FastAPI, ChromaDB, and Ollama**

[ğŸ“– Documentation](#-api-documentation) â€¢ [ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ› Issues](#-troubleshooting)

---

**â­ If this helped you, please star the repository!**

</div>
